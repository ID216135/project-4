# project-4
Group Members: Benafsha Alishah, Jonathan Bateson, Ian Denker, Maddie Haughton

Data Source: https://www.kaggle.com/datasets/shkarupylomaxim/chess-games-dataset-lichess-2017-may/data

Summary: We looked at a large dataset about online chess matches from Lichess in January 2024 and created multiple models to predict the outcome of a match. After creating hyperparameters to represent factors about both players, we did exploratory analysis with boxplots to determine if there were outliers in our dataset and to set our expectations about the capacity of any model to predict outcomes. Next, we created a logistic regression model to get a baseline accuracy score for the predictive power of models we would create. That accuracy score was ~54%. Then, we tried a random forest model that yielded an accuracy score of ~61%. We proceeded to create multiple neural network models which did not reach validation accuracy scores of above 58%. Our instructor, Mark Eidsaune, and TA, Jacob Johnson, suggested that we use XGBoost to optimize our predictions. We created multiple XGBoost models and found produce the best outcomes of any of our models. We then resampled the data so that the win, lose, and draw outcomes for the training data would be more comparable and also binned our data based on outliers in the rating difference feature. All this together yielded a model with an accuracy score of 91%. This process is shown in our file data_exploration.ipynb and is displayed in the presentation we created for our class in the file project 4 group 1.pdf.
